{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VA9-dgazNg-g"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import pickle\n",
        "import joblib\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfDLwBwHNg-k"
      },
      "source": [
        "# Content Based"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgDlLyXmNg-t",
        "outputId": "ef986634-10dc-4038-9bb3-459970a8d421"
      },
      "outputs": [],
      "source": [
        "\n",
        "content_based_data = pd.read_pickle('processed_df.pkl')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECMjaEi0Ng-0"
      },
      "source": [
        "# Collabative Filtring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7wpQ6ehNg-8",
        "outputId": "550caeb6-f36c-4609-fc52-ddacf6695d47"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load the ratings file\n",
        "colaborative_data = pd.read_csv('colaborative.csv')\n",
        "\n",
        "ratings_df = colaborative_data[['user_id', 'book_id', 'rating']]\n",
        "book_metadata = colaborative_data[['book_id', 'title', 'authors', 'categories']].drop_duplicates('book_id').set_index('book_id')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class BayesianSVD:\n",
        "    def __init__(self, n_factors=20, n_epochs=20, lr=0.008, reg=0.002,\n",
        "                 prior_mean=0, prior_std=0.1, content_weight=0.5, content_rate=0.1,\n",
        "                 lr_decay=0.99, clip_value=5.0):\n",
        "        self.n_factors = n_factors\n",
        "        self.n_epochs = n_epochs\n",
        "        self.lr = lr                # initial learning rate\n",
        "        self.reg = reg\n",
        "        self.prior_mean = prior_mean\n",
        "        self.prior_std = prior_std\n",
        "        self.content_weight = content_weight\n",
        "        self.content_rate = content_rate\n",
        "        # New hyperparameters for adaptive learning and gradient clipping\n",
        "        self.lr_decay = lr_decay    # factor to exponentially decay the learning rate each epoch\n",
        "        self.clip_value = clip_value\n",
        "\n",
        "        self.user_factors = None\n",
        "        self.item_factors = None\n",
        "        self.global_mean = None\n",
        "        self.user_biases = None\n",
        "        self.item_biases = None\n",
        "        self.user_mapping = None\n",
        "        self.item_mapping = None\n",
        "        self.item_features = None  # optional content features for items\n",
        "\n",
        "    def fit(self, ratings_df, item_features=None):\n",
        "        \"\"\"\n",
        "        Train the model given a ratings DataFrame.\n",
        "        Optionally provide item_features (a dictionary mapping book_id to content feature vector).\n",
        "        \"\"\"\n",
        "        # Create mappings from user and item IDs to indices.\n",
        "        unique_users = ratings_df['user_id'].unique()\n",
        "        unique_items = ratings_df['book_id'].unique()\n",
        "        self.user_mapping = {user: i for i, user in enumerate(unique_users)}\n",
        "        self.item_mapping = {item: i for i, item in enumerate(unique_items)}\n",
        "\n",
        "        # Convert ratings to internal indices.\n",
        "        user_indices = ratings_df['user_id'].map(self.user_mapping).values\n",
        "        item_indices = ratings_df['book_id'].map(self.item_mapping).values\n",
        "        ratings = ratings_df['rating'].values\n",
        "\n",
        "        # Global mean.\n",
        "        self.global_mean = np.mean(ratings)\n",
        "\n",
        "        n_users = len(unique_users)\n",
        "        n_items = len(unique_items)\n",
        "\n",
        "        # Initialize user factors and biases.\n",
        "        self.user_factors = np.random.normal(self.prior_mean, self.prior_std, (n_users, self.n_factors))\n",
        "        self.user_biases = np.zeros(n_users)\n",
        "\n",
        "        # Initialize item factors.\n",
        "        if item_features is not None:\n",
        "            self.item_features = item_features\n",
        "            self.item_factors = np.zeros((n_items, self.n_factors))\n",
        "            for item in unique_items:\n",
        "                idx = self.item_mapping[item]\n",
        "                if item in item_features:\n",
        "                    self.item_factors[idx] = (self.content_weight * np.array(item_features[item]) +\n",
        "                                              (1 - self.content_weight) * np.random.normal(self.prior_mean, self.prior_std, self.n_factors))\n",
        "                else:\n",
        "                    self.item_factors[idx] = np.random.normal(self.prior_mean, self.prior_std, self.n_factors)\n",
        "        else:\n",
        "            self.item_factors = np.random.normal(self.prior_mean, self.prior_std, (n_items, self.n_factors))\n",
        "        self.item_biases = np.zeros(n_items)\n",
        "\n",
        "        # Training loop.\n",
        "        for epoch in range(self.n_epochs):\n",
        "            # Adaptive learning rate: decaying exponentially.\n",
        "            current_lr = self.lr * (self.lr_decay ** epoch)\n",
        "            indices = np.arange(len(ratings))\n",
        "            np.random.shuffle(indices)\n",
        "\n",
        "            for idx in indices:\n",
        "                u = user_indices[idx]\n",
        "                i = item_indices[idx]\n",
        "                r = ratings[idx]\n",
        "\n",
        "                # Prediction.\n",
        "                pred = self.global_mean + self.user_biases[u] + self.item_biases[i] + \\\n",
        "                       np.dot(self.user_factors[u], self.item_factors[i])\n",
        "                err = r - pred\n",
        "\n",
        "                # Update biases (no clipping for biases, since they are scalars).\n",
        "                self.user_biases[u] += current_lr * (err - self.reg * self.user_biases[u])\n",
        "                self.item_biases[i] += current_lr * (err - self.reg * self.item_biases[i])\n",
        "\n",
        "                # Save a copy of the current user factors for use in the item update.\n",
        "                temp_user_factors = self.user_factors[u].copy()\n",
        "\n",
        "                # Compute gradients for user and item factors.\n",
        "                grad_user = err * self.item_factors[i] - self.reg * self.user_factors[u]\n",
        "                grad_item = err * temp_user_factors - self.reg * self.item_factors[i]\n",
        "\n",
        "                # Gradient clipping.\n",
        "                norm_grad_user = np.linalg.norm(grad_user)\n",
        "                if norm_grad_user > self.clip_value:\n",
        "                    grad_user = grad_user * (self.clip_value / norm_grad_user)\n",
        "                norm_grad_item = np.linalg.norm(grad_item)\n",
        "                if norm_grad_item > self.clip_value:\n",
        "                    grad_item = grad_item * (self.clip_value / norm_grad_item)\n",
        "\n",
        "                # Update latent factors with the current learning rate.\n",
        "                self.user_factors[u] += current_lr * grad_user\n",
        "                self.item_factors[i] += current_lr * grad_item\n",
        "\n",
        "            # Adaptive regularization: Bayesian prior pull.\n",
        "            self._apply_adaptive_regularization(epoch)\n",
        "            # Customized bias removal: Center item biases to reduce item bias dominance.\n",
        "            self._center_item_biases()\n",
        "\n",
        "            # Model inspection: print mean and std of latent factors.\n",
        "            user_factors_mean = np.mean(self.user_factors)\n",
        "            user_factors_std = np.std(self.user_factors)\n",
        "            item_factors_mean = np.mean(self.item_factors)\n",
        "            item_factors_std = np.std(self.item_factors)\n",
        "            print(f\"Epoch {epoch + 1}/{self.n_epochs} | LR: {current_lr:.5f} | \"\n",
        "                  f\"User Factors: mean={user_factors_mean:.4f}, std={user_factors_std:.4f} | \"\n",
        "                  f\"Item Factors: mean={item_factors_mean:.4f}, std={item_factors_std:.4f}\")\n",
        "\n",
        "    def _apply_adaptive_regularization(self, epoch):\n",
        "        # Decay the 'prior pull' over epochs.\n",
        "        initial_prior_weight = 0.1\n",
        "        final_prior_weight = 0.01\n",
        "        decay_rate = (initial_prior_weight - final_prior_weight) / self.n_epochs\n",
        "        prior_weight = initial_prior_weight - (epoch * decay_rate)\n",
        "\n",
        "        self.user_factors = (1 - prior_weight) * self.user_factors + prior_weight * self.prior_mean\n",
        "        self.item_factors = (1 - prior_weight) * self.item_factors + prior_weight * self.prior_mean\n",
        "\n",
        "        # If content features are available, further pull item factors toward their content vector.\n",
        "        if self.item_features is not None:\n",
        "            for item, idx in self.item_mapping.items():\n",
        "                if item in self.item_features:\n",
        "                    content_vector = np.array(self.item_features[item])\n",
        "                    self.item_factors[idx] = (1 - self.content_rate) * self.item_factors[idx] + \\\n",
        "                                             self.content_rate * content_vector\n",
        "\n",
        "    def _center_item_biases(self):\n",
        "        # Center item biases to remove systematic over-emphasis.\n",
        "        mean_item_bias = np.mean(self.item_biases)\n",
        "        self.item_biases -= mean_item_bias\n",
        "\n",
        "    def predict(self, user_id, book_id):\n",
        "        if user_id not in self.user_mapping or book_id not in self.item_mapping:\n",
        "            return self.global_mean\n",
        "\n",
        "        u = self.user_mapping[user_id]\n",
        "        i = self.item_mapping[book_id]\n",
        "        pred = self.global_mean + self.user_biases[u] + 0.2 * self.item_biases[i] + \\\n",
        "               np.dot(self.user_factors[u], self.item_factors[i])\n",
        "        # Ensure predicted rating is within the bounds, e.g. 1 to 5.\n",
        "        return max(1, min(5, pred))\n",
        "\n",
        "    def predict_for_user(self, user_id, book_ids):\n",
        "        predictions = []\n",
        "        for book_id in book_ids:\n",
        "            pred = self.predict(user_id, book_id)\n",
        "            predictions.append((book_id, pred))\n",
        "        return predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Load the augmented hybrid model\n",
        "with open(\"augmented_hybrid_model.pkl\", \"rb\") as f:\n",
        "    augmented_model = pickle.load(f)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gneGKlZM1Ox-",
        "outputId": "98c40ccd-0038-4296-9daf-f71bd34a5aa8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Book ID: B000MZW2AO, Title: Charlotte's Web, Authors: ['E. B. White'], Predicted Rating: 4.536\n",
            "Book ID: 0736694242, Title: Under the Banner of Heaven, Authors: ['Jon Krakauer'], Predicted Rating: 4.455\n",
            "Book ID: B000NPRINY, Title: All Creatures Great and Small, Authors: ['James Herriot'], Predicted Rating: 4.453\n",
            "Book ID: 0446781819, Title: Gone with the Wind, Authors: ['Margaret Mitchell'], Predicted Rating: 4.436\n",
            "Book ID: B000KHZ3QE, Title: The Great Divorce, Authors: ['C. S. Lewis'], Predicted Rating: 4.431\n",
            "Book ID: B000L346OM, Title: Man's Search for Meaning, Authors: ['Viktor E Frankl'], Predicted Rating: 4.418\n",
            "Book ID: 0460872702, Title: Great Gatsby (Everyman), Authors: ['F. Scott Fitzgerald'], Predicted Rating: 4.413\n",
            "Book ID: B0000632ZJ, Title: Lonesome Dove, Authors: ['Larry McMurtry'], Predicted Rating: 4.410\n",
            "Book ID: B000J1OR0Y, Title: The Lord of the Rings (3 Volume Set), Authors: ['J. R. R. Tolkien'], Predicted Rating: 4.409\n",
            "Book ID: B00017JIQE, Title: Under the Banner of Heaven: A Story of Violent Faith, Authors: ['Jon Krakauer'], Predicted Rating: 4.409\n",
            "Book ID: 0460112872, Title: Jane Eyre (Everyman's Classics), Authors: ['Charlotte Brontë'], Predicted Rating: 4.404\n",
            "Book ID: 0140292926, Title: The Grapes of Wrath (Steinbeck Essentials), Authors: ['John Steinbeck'], Predicted Rating: 4.402\n",
            "Book ID: 0140351310, Title: Jane Eyre: Complete and Unabridged (Puffin Classics), Authors: ['Charlote Brontë'], Predicted Rating: 4.400\n",
            "Book ID: B0000CO4JZ, Title: The Great Gatsby, Authors: ['F. Scott Fitzgerald'], Predicted Rating: 4.400\n",
            "Book ID: B000JJVHZE, Title: To Kill A Mockingbird, Authors: ['Harper Lee'], Predicted Rating: 4.395\n"
          ]
        }
      ],
      "source": [
        "def generate_recommendations_augmented(model, user_id, top_n=10, rated_books=None, metadata_df=None):\n",
        "    \"\"\"\n",
        "    Generate recommendations for a given user using the augmented BayesianSVD model.\n",
        "    Only items with available metadata will be shown.\n",
        "\n",
        "    Parameters:\n",
        "      model       : The trained augmented BayesianSVD model.\n",
        "      user_id     : The user ID for which to generate recommendations.\n",
        "      top_n       : Number of recommendations to return.\n",
        "      rated_books : An optional set (or list) of book IDs the user has already rated.\n",
        "      metadata_df : A DataFrame containing metadata for books having at least 'book_id', 'title', and 'authors'.\n",
        "\n",
        "    Returns:\n",
        "      A list of dictionaries, each containing:\n",
        "         - 'book_id'\n",
        "         - 'title'\n",
        "         - 'authors'\n",
        "         - 'predicted_rating'\n",
        "    \"\"\"\n",
        "    # Invert the item mapping: {index: book_id}\n",
        "    inv_item_mapping = {index: book for book, index in model.item_mapping.items()}\n",
        "    recommendations = []\n",
        "\n",
        "    # Iterate over all items in the model.\n",
        "    for i in range(len(model.item_mapping)):\n",
        "        book_id = inv_item_mapping[i]\n",
        "\n",
        "        # Skip if this book is already rated by the user.\n",
        "        if rated_books is not None and book_id in rated_books:\n",
        "            continue\n",
        "\n",
        "        # Look up metadata if the DataFrame is provided.\n",
        "        if metadata_df is not None:\n",
        "            meta_row = metadata_df.loc[metadata_df['book_id'] == book_id]\n",
        "            # Skip this candidate if metadata is missing.\n",
        "            if meta_row.empty:\n",
        "                continue\n",
        "            title = meta_row.iloc[0]['title']\n",
        "            authors = meta_row.iloc[0]['authors']\n",
        "        else:\n",
        "            # If no metadata dataframe is provided, skip candidate (or you could fill with default values)\n",
        "            continue\n",
        "\n",
        "        # Generate a predicted rating.\n",
        "        pred_rating = model.predict(user_id, book_id)\n",
        "        rec = {\n",
        "            'book_id': book_id,\n",
        "            'title': title,\n",
        "            'authors': authors,\n",
        "            'predicted_rating': pred_rating\n",
        "        }\n",
        "        recommendations.append(rec)\n",
        "\n",
        "    # Sort recommendations by predicted rating in descending order.\n",
        "    recommendations.sort(key=lambda x: x['predicted_rating'], reverse=True)\n",
        "    return recommendations[:top_n]\n",
        "\n",
        "\n",
        "# --- Example usage ---\n",
        "# Assume:\n",
        "# - `augmented_model` is your trained augmented BayesianSVD model.\n",
        "# - `ratings_df` is your ratings DataFrame (with at least 'user_id', 'book_id', 'rating').\n",
        "# - `content_based_data` (or another DataFrame) contains metadata with columns 'book_id', 'title', and 'authors'.\n",
        "\n",
        "user_id_example = 'A2F6N60Z96CAJI'\n",
        "\n",
        "# Collect the set of books the user has already rated.\n",
        "rated_books = set(ratings_df[ratings_df['user_id'] == user_id_example]['book_id'].tolist())\n",
        "\n",
        "# Generate recommendations; only candidates with metadata (i.e. known titles) will appear.\n",
        "top_recommendations = generate_recommendations_augmented(\n",
        "    model=augmented_model,\n",
        "    user_id=user_id_example,\n",
        "    top_n=15,\n",
        "    rated_books=rated_books,\n",
        "    metadata_df=content_based_data\n",
        ")\n",
        "\n",
        "# Display recommendations.\n",
        "for rec in top_recommendations:\n",
        "    print(f\"Book ID: {rec['book_id']}, Title: {rec['title']}, Authors: {rec['authors']}, \"\n",
        "          f\"Predicted Rating: {rec['predicted_rating']:.3f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
